# WaveQ - AI Audio Agent SDK/API

![Architecture](docs/architecture.png)

**WaveQ** is a comprehensive Python SDK and API for AI-powered audio processing. It provides an intelligent orchestrator that analyzes your audio needs and automatically applies the right combination of AI tools.

## âœ¨ Features

- ğŸ™ï¸ **AI Denoising & Enhancement** - Remove background noise and enhance audio quality
- ğŸ“ **Speech-to-Text with Diarization** - Transcribe audio with speaker identification
- âœ‚ï¸ **Smart Trimming** - Automatically remove silence and trim audio
- ğŸµ **Source Separation** - Separate vocals, drums, bass, and other instruments
- ğŸ˜Š **Sentiment Analysis** - Analyze emotions and sentiment in audio
- ğŸ—£ï¸ **Text-to-Speech** - Convert text to natural-sounding speech
- ğŸ¤– **AI Orchestrator** - Intelligent workflow management and intent classification

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/waveq.git
cd waveq

# Install dependencies
pip install -r requirements.txt

# Copy environment variables
cp .env.example .env

# Edit .env with your configuration
```

### Start the API Server

```bash
# Run with uvicorn
python main.py

# Or use uvicorn directly
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

The API will be available at `http://localhost:8000`

- **API Documentation**: http://localhost:8000/docs
- **API Redoc**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/health

### Using the SDK

```python
from waveq import WaveQClient

# Initialize client
client = WaveQClient(api_key="your-api-key")

# Denoise audio
result = client.denoise_audio("noisy_audio.wav")
print(f"Task ID: {result.task_id}")

# Transcribe with speaker diarization
result = client.transcribe_audio(
    "meeting.mp3",
    enable_diarization=True
)
print(f"Transcript: {result.transcript}")

# Text to speech
result = client.text_to_speech(
    text="Hello, this is WaveQ!",
    language="en"
)
```

## ğŸ“¦ Project Structure

```
waveq-python-api/
â”œâ”€â”€ waveq/                      # SDK package
â”‚   â”œâ”€â”€ __init__.py            # Package initialization
â”‚   â”œâ”€â”€ client.py              # Main SDK client
â”‚   â”œâ”€â”€ models.py              # Pydantic models
â”‚   â””â”€â”€ exceptions.py          # Custom exceptions
â”œâ”€â”€ api/                        # API server
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ routes.py              # API endpoints
â”‚   â””â”€â”€ auth.py                # Authentication
â”œâ”€â”€ orchestrator/               # AI orchestrator
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ orchestrator.py        # Workflow management
â”œâ”€â”€ audio_tools/                # Audio processing tools
â”‚   â”œâ”€â”€ denoiser.py
â”‚   â”œâ”€â”€ transcription.py
â”‚   â”œâ”€â”€ trimming.py
â”‚   â”œâ”€â”€ separator.py
â”‚   â”œâ”€â”€ sentiment.py
â”‚   â””â”€â”€ tts.py
â”œâ”€â”€ examples/                   # Example code
â”‚   â””â”€â”€ example_client.py
â”œâ”€â”€ main.py                     # FastAPI app entry
â”œâ”€â”€ config.py                   # Configuration
â”œâ”€â”€ utils.py                    # Utilities
â””â”€â”€ requirements.txt            # Dependencies
```

## ğŸ”§ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v1/denoise` | POST | Remove noise from audio |
| `/api/v1/transcribe` | POST | Transcribe audio to text |
| `/api/v1/trim` | POST | Smart trimming and silence removal |
| `/api/v1/separate` | POST | Separate audio sources |
| `/api/v1/sentiment` | POST | Analyze audio sentiment |
| `/api/v1/tts` | POST | Text-to-speech conversion |
| `/api/v1/tasks/{task_id}` | GET | Get task status |

## ğŸ¯ Use Cases

### Podcast Production
```python
# Automatically denoise, trim, and transcribe
client.denoise_audio("podcast_raw.wav")
client.trim_audio("podcast_raw.wav")
client.transcribe_audio("podcast_raw.wav", enable_diarization=True)
```

### Music Production
```python
# Separate vocals from instrumentals
result = client.separate_audio(
    "song.mp3",
    separation_type="vocals"
)
```

### Call Center Analytics
```python
# Transcribe and analyze sentiment
transcription = client.transcribe_audio("call.wav")
sentiment = client.analyze_sentiment("call.wav", include_emotions=True)
```

## ğŸ” Authentication

The API uses Bearer token authentication. Include your API key in requests:

```bash
curl -X POST "http://localhost:8000/api/v1/denoise" \
  -H "Authorization: Bearer your-api-key" \
  -F "audio_file=@audio.wav"
```

## ğŸ³ Docker Deployment

```bash
# Build image
docker build -t waveq-api .

# Run container
docker run -p 8000:8000 waveq-api
```

## âš™ï¸ Configuration

Edit `.env` file to configure:

- **API Settings**: Port, debug mode, CORS origins
- **AI Models**: Whisper model size, device (CPU/GPU), Demucs model
- **Database**: Database URL for task storage
- **Third-party APIs**: OpenAI, ElevenLabs API keys

## ğŸ“Š Monitoring

Check server logs and task statuses:

```python
# Get task status
status = client.get_task_status(task_id)
print(f"Status: {status.status}")
print(f"Progress: {status.metadata}")
```

## ğŸ› ï¸ Development

### Running Tests

```bash
pytest tests/ -v
```

### Code Quality

```bash
# Format code
black .

# Lint
flake8 .

# Type checking
mypy .
```

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Support

For issues and questions, please open an issue on GitHub or contact support@waveq.ai

---

**Built with â¤ï¸ using FastAPI, OpenAI Whisper, and Demucs**
